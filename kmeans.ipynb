{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have the same content as the single-run kmeans file but in a jupyter notebook format it'll be easier to run and manage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd  \n",
    "from scipy import stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter \n",
    "import tqdm\n",
    "\n",
    "events = 10 \n",
    "density = '1' \n",
    "noise = 0 \n",
    "filename = None \n",
    "folder = None \n",
    "k = 100\n",
    "max_iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_gpu(data, k, max_iters=100):\n",
    "    data = data.to(device)\n",
    "    centroids = data[torch.randperm(len(data))[:k]]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        #print(f\"Iteration: {i+1}\")\n",
    "        dist = torch.cdist(data, centroids) \n",
    "        ai_labels = torch.argmin(dist, dim=1)\n",
    "\n",
    "        # update centroids \n",
    "        for j in range(k):\n",
    "            centroids[j] = data[ai_labels == j].mean(dim=0)\n",
    "    \n",
    "    return ai_labels.cpu().numpy(), centroids.numpy()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelmaker(events=None, sp_density=None, t_density=None, noise=None, filename = None, folder = None): \n",
    "    '''creates labels based on my naming convention for different files, keeps it consistent and easy'''\n",
    "    if folder: \n",
    "        folder = folder + '/'\n",
    "\n",
    "    if filename: \n",
    "        datafile = str(filename) \n",
    "    else: \n",
    "        datafile = str(events) + 'ev_' + str(sp_density) + 'spd_' + str(t_density) + 'td_n' + str(noise)\n",
    "    \n",
    "\n",
    "    labelfile = 'labels_' + datafile + '.csv'\n",
    "    sourcefile = 'sources_' + datafile + '.csv'\n",
    "    ai_labelfile = datafile + '_results' + '.csv'\n",
    "    centroidfile = datafile + '_centroids' + '.csv'\n",
    "    datafile = datafile + '.csv'\n",
    "\n",
    "    if folder: \n",
    "        datafile = folder + datafile \n",
    "        centroidfile = folder + centroidfile \n",
    "        ai_labelfile = folder + ai_labelfile \n",
    "        labelfile = folder + labelfile \n",
    "        sourcefile = folder + sourcefile \n",
    "\n",
    "    \n",
    "    return datafile, labelfile, sourcefile, ai_labelfile, centroidfile\n",
    "\n",
    "def readfiles(datafile, labelfile, sourcefile): \n",
    "    '''data reader and simplifier for files that haven't been passed through the algorithm'''\n",
    "    columns = ['x[px]', 'y[px]', 't[s]']\n",
    "    \n",
    "    dataread = pd.read_csv(datafile) \n",
    "    data = np.array(dataread[columns])\n",
    "    \n",
    "    labelread = pd.read_csv(labelfile)\n",
    "    labels = np.array(labelread['labels'])\n",
    "\n",
    "    sourceread = pd.read_csv(sourcefile) \n",
    "    sources = np.array(sourceread[columns])\n",
    "\n",
    "    return data, labels, sources\n",
    "\n",
    "def readai(datafile, labelfile, sourcefile, ai_labelfile, centroidfile):\n",
    "    '''file reader and data simplifier for data thats been through the algorithm'''\n",
    "    columns = ['x[px]', 'y[px]', 't[s]']\n",
    "    \n",
    "    dataread = pd.read_csv(datafile) \n",
    "    data = np.array(dataread[columns])\n",
    "    \n",
    "    labelread = pd.read_csv(labelfile)\n",
    "    labels = np.array(labelread['labels'])\n",
    "\n",
    "    sourceread = pd.read_csv(sourcefile) \n",
    "    sources = np.array(sourceread[columns])\n",
    "\n",
    "    ai_labelsread = pd.read_csv(ai_labelfile)\n",
    "    ai_labels = np.array(ai_labelsread['labels'])\n",
    "\n",
    "    centroidread = pd.read_csv(centroidfile) \n",
    "    centroids = np.array(centroidread[columns])\n",
    "\n",
    "    return data, labels, sources, ai_labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating datafile names and data + normalizing it using MinMaxScaling below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafile, labelfile, sourcefile, ai_labelfile, centroidfile = labelmaker(events, density, noise, filename, folder)\n",
    "# # read out the files\n",
    "# data, labels, sources = readfiles(datafile, labelfile, sourcefile)\n",
    "# # establish feature range and transform data, can comment this out to turn off scaling, naming is the same\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# # scale coordinate data \n",
    "# data = scaler.fit_transform(data)\n",
    "# sources = scaler.fit_transform(sources)\n",
    "# # turn data into a tensor \n",
    "# data_tensor = torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_labels, centroids = kmeans_gpu(data_tensor, k, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to the centroid and ai_label files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(ai_labelfile, mode = 'w', newline='') as wfile: \n",
    "#     writer = csv.writer(wfile)\n",
    "#     writer.writerow(['labels'])\n",
    "#     for item in ai_labels: \n",
    "#                 writer.writerow([item])\n",
    "\n",
    "# columns = ['x[px]', 'y[px]', 't[s]']\n",
    "# with open(centroidfile, mode = 'w', newline = '') as wfile: \n",
    "#     writer = csv.writer(wfile)\n",
    "#     writer.writerow(columns)\n",
    "#     writer.writerows(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading out ai generated files after the algorithm run \n",
    "\n",
    "data, labels, sources, ai_labels, centroids = readai(datafile, labelfile, sourcefile, ai_labelfile, centroidfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaks and sizing functions for the true label-focused loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaks(array): \n",
    "    '''This function takes an array, goes through it item by item, and returns the list of indices where the value changes'''\n",
    "    value = array[0] \n",
    "    indices = []\n",
    "    for index, ele in enumerate(array): \n",
    "        if ele != value:\n",
    "            indices.append(index)\n",
    "            value = array[index]\n",
    "    indices.append(len(array))\n",
    "\n",
    "    return indices\n",
    "\n",
    "def sizes(indices): \n",
    "    '''This function takes a list of indices and calculates the number of items belonging to each value by taking the difference between \n",
    "    subsequent indices'''\n",
    "    gaps = []\n",
    "    prev=0\n",
    "    for i in range(len(indices)):\n",
    "        gaps.append(indices[i]-prev)\n",
    "        prev = indices[i]\n",
    "\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Label-Focused Loss, ripped from the losses file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional function needed for the loss function \n",
    "def modded_mode(array): \n",
    "    '''This function will return event labels, its a modification on a normal mode function where the label has to appear at least 33% of the time to be a label in that chunk.'''\n",
    "    n = len(array)\n",
    "    if n == 0: \n",
    "        return []\n",
    "    \n",
    "    threshold = n/3 \n",
    "    counts = Counter(array)\n",
    "\n",
    "    result = [key for key, count in counts.items() if count >= threshold]\n",
    "    return result \n",
    "\n",
    "# THE loss func. \n",
    "\n",
    "def truth_based_loss(true_labels, ai_labels):\n",
    "    # labels are already sorted by true labels predictions \n",
    "\n",
    "    # getting break indices and cluster sizes \n",
    "    break_indices = breaks(true_labels)\n",
    "    gaps = sizes(break_indices)\n",
    "    n_events = len(gaps)\n",
    "\n",
    "    # initialize variables \n",
    "    counter = Counter()\n",
    "    fractions_misIDs = []\n",
    "    total_splits, ev_per_split = 0,0 \n",
    "    total_splits = 0 \n",
    "    total_misIDs = 0 \n",
    "\n",
    "\n",
    "    # process each chunk that's separated by truth gaps \n",
    "    for start, end, gap in zip([0] + break_indices[:-1], break_indices, gaps):\n",
    "        chunk = ai_labels[start:end]\n",
    "        chunk_modes = modded_mode(chunk)\n",
    "\n",
    "        # update counter\n",
    "        counter.update(chunk_modes)\n",
    "    \n",
    "        e_in_split = len(chunk_modes) # number of ai events found in the chunk\n",
    "        if e_in_split > 1: \n",
    "            total_splits += 1\n",
    "            ev_per_split += e_in_split # if there is more than one, increase splits and events in split \n",
    "        \n",
    "        misIDs = sum(1 for item in chunk if item not in chunk_modes)\n",
    "        total_misIDs += misIDs\n",
    "        fractions_misIDs.append(misIDs/gap)\n",
    "    \n",
    "    # Combination Statistics \n",
    "    repeat_labels = {k: v for k, v in counter.items() if v>1}\n",
    "    total_combos = len(repeat_labels) # the amount of combinations is the same as the amount of labels that get repeated through the set \n",
    "    ev_per_combo = sum(repeat_labels.values())/total_combos if total_combos else 0 # sum all repeats together and average over number of combinations \n",
    "    frac_combos = total_combos / n_events # fraction of events experiencing combination\n",
    "    \n",
    "    # other stats \n",
    "    ev_per_split = ev_per_split/total_splits if total_splits else 0\n",
    "    frac_splits = total_splits/n_events\n",
    "    avg_misIDs = np.mean(fractions_misIDs)\n",
    "\n",
    "    # output da resultssss\n",
    "    # print(f\"The fraction of splits over all events is {frac_splits}\")\n",
    "    # print(f\"The average number of events involved in a single split is {ev_per_split}\")\n",
    "    # print(f\"The fraction of combinations over all events is {frac_combos}\")\n",
    "    # print(f\"The average number of events involved in a single combo is {ev_per_combo}\")\n",
    "    # print(f\"The average fraction of photons misidentified in each event is {avg_misIDs}\")\n",
    "\n",
    "    return frac_splits, ev_per_split, frac_combos, ev_per_combo, avg_misIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Label-Focused Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these results should match up exactly with the previous loss function within rounding errors \n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def ai_based_loss(true_labels, network_labels):\n",
    "    \"\"\"\n",
    "    More efficient implementation of the AI-based loss function.\n",
    "    This function clusters AI labels and calculates relevant loss statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort labels based on AI predictions\n",
    "    sorted_pairs = sorted(zip(network_labels, true_labels))\n",
    "    reo_network_labels, reo_true_labels = zip(*sorted_pairs)\n",
    "\n",
    "    # Get break indices and cluster sizes\n",
    "    break_indices = breaks(reo_network_labels)\n",
    "    gaps = sizes(break_indices)\n",
    "    # n_events = len(gaps)\n",
    "\n",
    "    # Initialize variables\n",
    "    counter = Counter()\n",
    "    fractions_misIDs = []\n",
    "    total_combos, ev_per_combo = 0, 0\n",
    "    total_splits = 0\n",
    "    total_misIDs = 0 \n",
    "\n",
    "    # Process each AI-clustered chunk\n",
    "    for start, end, gap in zip([0] + break_indices[:-1], break_indices, gaps):\n",
    "        chunk = reo_true_labels[start:end]\n",
    "        chunk_modes = modded_mode(chunk)  # Get dominant modes (max 3)\n",
    "        \n",
    "        # Update Counter directly  \n",
    "        counter.update(chunk_modes)\n",
    "\n",
    "        e_in_combo = len(chunk_modes)\n",
    "        if e_in_combo > 1:\n",
    "            total_combos += 1\n",
    "            ev_per_combo += e_in_combo\n",
    "        \n",
    "        misIDs = sum(1 for item in chunk if item not in chunk_modes)\n",
    "        total_misIDs += misIDs\n",
    "        fractions_misIDs.append(misIDs / gap)\n",
    "    \n",
    "    n_events = len(counter)\n",
    "    # Compute split statistics\n",
    "    repeat_labels = {k: v for k, v in counter.items() if v > 1}\n",
    "    total_splits = len(repeat_labels)\n",
    "    ev_per_split = sum(repeat_labels.values()) / total_splits if total_splits else 0\n",
    "    frac_splits = total_splits / n_events\n",
    "\n",
    "    # Compute final metrics\n",
    "    ev_per_combo = ev_per_combo / total_combos if total_combos else 0\n",
    "    frac_combos = total_combos / n_events\n",
    "    avg_misIDs = np.mean(fractions_misIDs)\n",
    "\n",
    "\n",
    "    # Output results\n",
    "    print(f\"The fraction of splits over all events is {frac_splits}\")\n",
    "    print(f\"The average number of events involved in a single split is {ev_per_split}\")\n",
    "    print(f\"The fraction of combinations over all events is {frac_combos}\")\n",
    "    print(f\"The average number of events involved in a single combo is {ev_per_combo}\")\n",
    "    print(f\"The average fraction of photons misidentified in each event is {avg_misIDs}\")\n",
    "\n",
    "    return frac_splits, ev_per_split, frac_combos, ev_per_combo, avg_misIDs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- TRUTH BASED LOSS: ----\n",
      "The fraction of splits over all events is 0.0\n",
      "The average number of events involved in a single split is 0\n",
      "The fraction of combinations over all events is 0.0\n",
      "The average number of events involved in a single combo is 0\n",
      "The average number of photons misidentified in each event is 0.9208333333333334\n",
      "---- AI BASED LOSS: ----\n",
      "The fraction of splits over all events is 0.1\n",
      "The average number of events involved in a single split is 10.3\n",
      "The fraction of combinations over all events is 0.03\n",
      "The average number of events involved in a single combo is 2.0\n",
      "The average number of photons misidentified in each event is 0.009357142857142857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1, 10.3, 0.03, 2.0, 0.009357142857142857)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing to compare ai and truth based loss functions \n",
    "print(\"---- TRUTH BASED LOSS: ----\")\n",
    "truth_based_loss(labels, ai_labels)\n",
    "print(\"---- AI BASED LOSS: ----\")\n",
    "ai_based_loss(labels, ai_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fraction of splits over all events is 0.3333333333333333\n",
      "The average number of events involved in a single split is 2.0\n",
      "The fraction of combinations over all events is 0.3333333333333333\n",
      "The average number of events involved in a single combo is 2.0\n",
      "The average number of photons misidentified in each event is 0.13888888888888887\n",
      "----\n",
      "The fraction of splits over all events is 0.3333333333333333\n",
      "The average number of events involved in a single split is 2.0\n",
      "The fraction of combinations over all events is 0.3333333333333333\n",
      "The average number of events involved in a single combo is 2.0\n",
      "The average number of photons misidentified in each event is 0.13888888888888887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 2.0, 0.3333333333333333, 2.0, 0.13888888888888887)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test dataset for loss function testing \n",
    "\n",
    "true_labelz = np.array([1,1,1,1,2,2,3,3,3,3,3,3])\n",
    "ai_labelz = np.array([8,55,55,55,55,55,101,101,8,8,55,8])\n",
    "\n",
    "truth_based_loss(true_labelz, ai_labelz)\n",
    "print(\"----\")\n",
    "ai_based_loss(true_labelz, ai_labelz)\n",
    "\n",
    "# (8, 8, 8, 8, 55, 55, 55, 55, 55, 55, 101, 101)\n",
    "# (1, 3, 3, 3, 1, 1, 1, 2, 2, 3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBOW METHOD \n",
    "def compute_loss(args):\n",
    "    \"\"\"Standalone function for parallel execution.\"\"\"\n",
    "    kval, data_tensor, labels, coeff, iterations = args\n",
    "    ai_labels, centroids = kmeans_gpu(data_tensor, k=kval, max_iters=iterations)\n",
    "    frac_splits, ev_per_split, frac_combos, ev_per_combo, avg_misIDs = truth_based_loss(true_labels=labels, ai_labels=ai_labels)\n",
    "    total_loss = np.dot(coeff, [avg_misIDs, frac_splits, frac_combos])\n",
    "    return kval, total_loss\n",
    "\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "def elbowmeth(int_range, datafile, folder=None, iterations=None, pltfile=None, pltnum=None, weighted = True):\n",
    "    '''will perform the elbow method on a given dataset for a given range of k integers and return the k value with the lowest loss, as well as the value of that loss\n",
    "    and a plot demonstrating the loss'''\n",
    "    datafile, labelfile, sourcefile, ai_labelfile, centroidfile = labelmaker(filename = datafile, folder = folder)\n",
    "    data, labels, sources = readfiles(datafile, labelfile, sourcefile)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    # scale coordinate data \n",
    "    data = scaler.fit_transform(data)\n",
    "    sources = scaler.fit_transform(sources)\n",
    "    # turn data into a tensor \n",
    "    data_tensor = torch.Tensor(data)    \n",
    "\n",
    "    coeff = [1,2,3] if weighted else [1,1,1]\n",
    "\n",
    "    args_list = [(k, data_tensor, labels, coeff, iterations) for k in int_range]\n",
    "\n",
    "    with mp.Pool(processes=len(int_range)) as pool:\n",
    "        results = list(pool.imap(compute_loss, args_list))\n",
    "\n",
    "    results.sort()  \n",
    "    k_values, total_losses = zip(*results)\n",
    "\n",
    "    if pltnum%5 == 0:\n",
    "        #generate plot for every fifth iteration\n",
    "        plt.figure()\n",
    "        plt.plot(int_range, total_losses)\n",
    "        plt.xlabel(\"K values\")\n",
    "        plt.ylabel(\"Total loss\")\n",
    "        plt.title(f\"Elbow method for k = {int_range} for {datafile} {pltnum}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(pltfile)        \n",
    "\n",
    "    min_loss = np.min(total_losses)\n",
    "    min_k = k_values[total_losses.index(min_loss)]\n",
    "\n",
    "    print(f\"The minimum loss is {min_loss} with a k value of {min_k}\")\n",
    "    return min_loss, min_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_sim import new_parallel_sim # this only works if parsed arguments in segmentation_sim.py are commented out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.002053388090349076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "0.002053388090349076\n"
     ]
    }
   ],
   "source": [
    "new_parallel_sim(1, 0, 1, dataSaveID = 'simcheck')\n",
    "new_parallel_sim(1, 0, 1, dataSaveID = 'simcheck2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0141421356237309490.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.20533880903490760.0141421356237309490.014142135623730949\n",
      "0.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.20533880903490760.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.0141421356237309490.2053388090349076\n",
      "\n",
      "0.20533880903490760.0141421356237309490.2053388090349076\n",
      "0.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "0.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.20533880903490760.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.20533880903490760.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.20533880903490760.0141421356237309490.014142135623730949\n",
      "0.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.2053388090349076\n",
      "0.0141421356237309490.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "0.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.014142135623730949\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.014142135623730949\n",
      "0.0141421356237309490.2053388090349076\n",
      "\n",
      "0.0141421356237309490.20533880903490760.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "0.014142135623730949\n",
      "0.0141421356237309490.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0th datafile generated!\n",
      "0% complete!\n",
      "The minimum loss is 0.760101484369734 with a k value of 105\n",
      "0th losses and k values are: 0.760101484369734, 105\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "0.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.0141421356237309490.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "0.014142135623730949\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.0141421356237309490.0141421356237309490.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.014142135623730949\n",
      "0.20533880903490760.014142135623730949\n",
      "0.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.014142135623730949\n",
      "0.0141421356237309490.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.014142135623730949\n",
      "0.0141421356237309490.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.0141421356237309490.014142135623730949\n",
      "0.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.0141421356237309490.0141421356237309490.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.014142135623730949\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.0141421356237309490.2053388090349076\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.014142135623730949\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.0141421356237309490.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "0.014142135623730949\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "1th datafile generated!\n",
      "The minimum loss is 0.760101484369734 with a k value of 105\n",
      "1th losses and k values are: 0.760101484369734, 105\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.0141421356237309490.2053388090349076\n",
      "0.20533880903490760.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.20533880903490760.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.014142135623730949\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "0.014142135623730949\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.20533880903490760.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.0141421356237309490.2053388090349076\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0.014142135623730949\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.0141421356237309490.2053388090349076\n",
      "0.014142135623730949\n",
      "0.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.0141421356237309490.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.014142135623730949\n",
      "0.20533880903490760.014142135623730949\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.014142135623730949\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.014142135623730949\n",
      "0.0141421356237309490.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.0141421356237309490.2053388090349076\n",
      "0.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "2th datafile generated!\n",
      "The minimum loss is 0.760101484369734 with a k value of 105\n",
      "2th losses and k values are: 0.760101484369734, 105\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.0141421356237309490.014142135623730949\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "0.014142135623730949\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.0141421356237309490.014142135623730949\n",
      "\n",
      "0.014142135623730949\n",
      "0.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.20533880903490760.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.014142135623730949\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "\n",
      "\n",
      "0.20533880903490760.014142135623730949\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.014142135623730949\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "0.2053388090349076\n",
      "\n",
      "0.20533880903490760.0141421356237309490.0141421356237309490.014142135623730949\n",
      "\n",
      "\n",
      "\n",
      "0.2053388090349076\n",
      "0.20533880903490760.2053388090349076\n",
      "\n",
      "0.014142135623730949\n",
      "0.0141421356237309490.0141421356237309490.2053388090349076\n",
      "\n",
      "0.2053388090349076\n",
      "0.2053388090349076\n",
      "\n",
      "0.0141421356237309490.014142135623730949\n",
      "\n",
      "0.20533880903490760.2053388090349076\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-130:\n",
      "Process ForkPoolWorker-132:\n",
      "Process ForkPoolWorker-129:\n",
      "Process ForkPoolWorker-131:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-133:\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/pool.py\", line 51, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/cgillesp/Downloads/segmentation_sim.py\", line 157, in sim\n",
      "    coords[2] = (generate_time(file = file) + start_time)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/queues.py\", line 387, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/Downloads/segmentation_sim.py\", line 157, in sim\n",
      "    coords[2] = (generate_time(file = file) + start_time)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/cgillesp/Downloads/segmentation_sim.py\", line 106, in generate_time\n",
      "    if photons_guess <= decayfit(time_guess, file = file):\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/Downloads/segmentation_sim.py\", line 106, in generate_time\n",
      "    if photons_guess <= decayfit(time_guess, file = file):\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/home/cgillesp/Downloads/segmentation_sim.py\", line 89, in decayfit\n",
      "    return a1*np.exp(-t/t1) + a2*np.exp(-t/t2) + a3*np.exp(-t/t3) + a4*np.exp(-t/t4)\n",
      "                                                                       ^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "           ^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/connection.py\", line 395, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/Downloads/segmentation_sim.py\", line 89, in decayfit\n",
      "    return a1*np.exp(-t/t1) + a2*np.exp(-t/t2) + a3*np.exp(-t/t3) + a4*np.exp(-t/t4)\n",
      "                                                    ^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/queues.py\", line 386, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# big algo \n",
    "\n",
    "k_range = [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n",
    "bigLossFile = '0.5temp100x100/loss_totals.csv'\n",
    "columns = ['lowest kval', 'min loss']\n",
    "\n",
    "\n",
    "with open(bigLossFile, mode = 'w', newline = '') as wfile: \n",
    "    writer = csv.writer(wfile)\n",
    "    writer.writerow(columns) # write the columns into the loss file to begin with \n",
    "\n",
    "for i in range(100): # 100 iterations to get good statistics \n",
    "    new_parallel_sim(100, 0, 5, t_density = '.5', folder = '0.5temp100x100', dataSaveID = 'temp') # generate data of 100 events, no noise, full density, using 5 cores for speed \n",
    "    print(f\"{i}th datafile generated!\")\n",
    "    min_loss, min_k = elbowmeth(k_range, datafile = 'temp', folder = 'temp100x100', iterations = 100, pltfile = f'0.5temp100x100/kvals_losses_{i}.png', pltnum = i) # run elbow meth!\n",
    "    print(f\"{i}th losses and k values are: {min_loss}, {min_k}\")\n",
    "    with open(bigLossFile, mode = 'a', newline = '') as wfile: # write in the best loss and best kval \n",
    "        writer = csv.writer(wfile)\n",
    "        writer.writerow([min_k, min_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
