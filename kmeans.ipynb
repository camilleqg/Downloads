{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have the same content as the single-run kmeans file but in a jupyter notebook format it'll be easier to run and manage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd  \n",
    "from scipy import stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter \n",
    "\n",
    "events = None \n",
    "density = None \n",
    "noise = None \n",
    "filename = None \n",
    "folder = None \n",
    "k = 100\n",
    "max_iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_gpu(data, k, max_iters=100):\n",
    "    data = data.to(device)\n",
    "    centroids = data[torch.randperm(len(data))[:k]]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        print(f\"Iteration: {i+1}\")\n",
    "        dist = torch.cdist(data, centroids) \n",
    "        ai_labels = torch.argmin(dist, dim=1)\n",
    "\n",
    "        # update centroids \n",
    "        for j in range(k):\n",
    "            centroids[j] = data[ai_labels == j].mean(dim=0)\n",
    "    \n",
    "    return ai_labels.cpu().numpy(), centroids.numpy()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelmaker(events, density, noise, filename = None, folder = None): \n",
    "    '''creates labels based on my naming convention for different files, keeps it consistent and easy'''\n",
    "    if folder: \n",
    "        folder = folder + '/'\n",
    "\n",
    "    if filename: \n",
    "        datafile = str(filename) \n",
    "    else: \n",
    "        datafile = str(events) + 'ev_' + str(density) + 'dense_n' + str(noise)\n",
    "    \n",
    "    labelfile = 'labels_' + datafile + '.csv'\n",
    "    sourcefile = 'sources_' + datafile + '.csv'\n",
    "    ai_labelfile = datafile + '_results' + '.csv'\n",
    "    centroidfile = datafile + '_centroids' + '.csv'\n",
    "\n",
    "    if folder: \n",
    "        datafile = folder + datafile \n",
    "        centroidfile = folder + centroidfile \n",
    "        ai_labelfile = folder + ai_labelfile \n",
    "        labelfile = folder + labelfile \n",
    "        sourcefile = folder + sourcefile \n",
    "\n",
    "    return datafile, labelfile, sourcefile, ai_labelfile, centroidfile\n",
    "\n",
    "def readfiles(datafile, labelfile, sourcefile): \n",
    "    '''data reader and simplifier for files that haven't been passed through the algorithm'''\n",
    "    columns = ['x[px]', 'y[px]', 't[s]']\n",
    "    \n",
    "    dataread = pd.read_csv(datafile) \n",
    "    data = np.array(dataread[columns])\n",
    "    \n",
    "    labelread = pd.read_csv(labelfile)\n",
    "    labels = np.array(labelread['labels'])\n",
    "\n",
    "    sourceread = pd.read_csv(sourcefile) \n",
    "    sources = np.array(sourceread[columns])\n",
    "\n",
    "    return data, labels, sources\n",
    "\n",
    "def readai(datafile, labelfile, sourcefile, ai_labelfile, centroidfile):\n",
    "    '''file reader and data simplifier for data thats been through the algorithm'''\n",
    "    columns = ['x[px]', 'y[px]', 't[s]']\n",
    "    \n",
    "    dataread = pd.read_csv(datafile) \n",
    "    data = np.array(dataread[columns])\n",
    "    \n",
    "    labelread = pd.read_csv(labelfile)\n",
    "    labels = np.array(labelread['labels'])\n",
    "\n",
    "    sourceread = pd.read_csv(sourcefile) \n",
    "    sources = np.array(sourceread[columns])\n",
    "\n",
    "    ai_labelsread = pd.read_csv(ai_labelfile)\n",
    "    ai_labels = np.array(ai_labelsread['labels'])\n",
    "\n",
    "    centroidread = pd.read_csv(centroidfile) \n",
    "    centroids = np.array(centroidread[columns])\n",
    "\n",
    "    return data, labels, sources, ai_labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating datafile names and data + normalizing it using MinMaxScaling below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile, labelfile, sourcefile, ai_labelfile, centroidfile = labelmaker(events, density, noise, filename, folder)\n",
    "# read out the files\n",
    "data, labels, sources = readfiles(datafile, labelfile, sourcefile)\n",
    "# establish feature range and transform data, can comment this out to turn off scaling, naming is the same\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data = scaler.fit_transform(data)\n",
    "sources = scaler.fit_transform(sources)\n",
    "# turn into a tensor \n",
    "data_tensor = torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_labels, centroids = kmeans_gpu(data_tensor, k, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to the centroid and ai_label files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ai_labelfile, mode = 'w', newline='') as wfile: \n",
    "    writer = csv.writer(wfile)\n",
    "    writer.writerow(['labels'])\n",
    "    for item in ai_labels: \n",
    "                writer.writerow([item])\n",
    "\n",
    "columns = ['x[px]', 'y[px]', 't[s]']\n",
    "with open(centroidfile, mode = 'w', newline = '') as wfile: \n",
    "    writer = csv.writer(wfile)\n",
    "    writer.writerow(columns)\n",
    "    writer.writerows(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaks and sizing functions for the true label-focused loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaks(array): \n",
    "    '''This function takes an array, goes through it item by item, and returns the list of indices where the value changes'''\n",
    "    value = array[0] \n",
    "    indices = []\n",
    "    for index, ele in enumerate(array): \n",
    "        if ele != value:\n",
    "            indices.append(index)\n",
    "            value = array[index]\n",
    "    indices.append(len(array))\n",
    "\n",
    "    return indices\n",
    "\n",
    "def sizes(indices): \n",
    "    '''This function takes a list of indices and calculates the number of items belonging to each value by taking the difference between \n",
    "    subsequent indices'''\n",
    "    gaps = []\n",
    "    prev=0\n",
    "    for i in range(len(indices)):\n",
    "        gaps.append(indices[i]-prev)\n",
    "        prev = indices[i]\n",
    "\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Label-Focused Loss, ripped from the losses file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional function needed for the loss function \n",
    "def modded_mode(array): \n",
    "    '''This function will return event labels, its a modification on a normal mode function where the label has to appear at least 33% of the time to be a label in that chunk.'''\n",
    "    n = len(array)\n",
    "    if n == 0: \n",
    "        return []\n",
    "    \n",
    "    threshold = n/3 \n",
    "    counts = Counter(array)\n",
    "\n",
    "    result = [key for key, count in counts.items() if count > threshold]\n",
    "    return result \n",
    "\n",
    "# THE loss func. \n",
    "\n",
    "def loss(true_labels, network_labels):\n",
    "    all_modes =[]\n",
    "    event_splits = 0 \n",
    "    all_misidentified = [] # array for all fractions of misidentified events \n",
    "\n",
    "    break_indices = breaks(true_labels)\n",
    "    gaps = sizes(break_indices) # sizes of each event \n",
    "\n",
    "    beginning = 0 \n",
    "    for i in range(len(gaps)):\n",
    "        misidentified=0 # number of misidentified photons per cluster starts at 0\n",
    "        end = break_indices[i] # determine where the cluster ends \n",
    "        chunk = network_labels[beginning:end] \n",
    "        chunk_modes = modded_mode(chunk) # find the modes, these are the ai labels given to the cluster, what events are dominant here \n",
    "        all_modes.extend(chunk_modes) # add to master list of modes, this indicates when each event label shows up (how often)\n",
    "        if len(chunk_modes) > 1: \n",
    "            event_splits += 1  # add one to a split. if there is more than one ai label in this list, theres a split cluster \n",
    "\n",
    "        chunk_modes_set = chunk_modes # change the list of modes into a set to increase efficiency\n",
    "        misidentified = sum(1 for item in chunk if item not in chunk_modes_set) # counts the number of photons not included in the main event labels (modes)\n",
    "        err_fraction = misidentified/gaps[i] # calculates the fraction of misidentified over number of photons in the event \n",
    "        all_misidentified.append(err_fraction) # adds the misidentification error to array \n",
    "        beginning = break_indices[i] # adjust the beginning of the next chunk\n",
    "    \n",
    "    avg_misidentified = np.average(all_misidentified)\n",
    "    counter = Counter(all_modes)\n",
    "    unfiltered_counts = dict(counter) # look at how many times each mode or \"label\" shows up in this list \n",
    "    mode_modes = {item: count for item, count in counter.items() if count > 1} # if it's more than once then an event has been combined \n",
    "    combo_frac = len(mode_modes.values())/len(unfiltered_counts.values())\n",
    "    avg_ev_in_combo = sum(mode_modes.values())/len(mode_modes.values())\n",
    "\n",
    "    print(f\"The fraction of misidentified photons in each event is: {all_misidentified}\")\n",
    "    print(f\"the average fraction of misidentified photons is: {avg_misidentified}\") \n",
    "    print(f\"The total number of event splits is {event_splits}\")\n",
    "    print(f\"The full list of events that were combined is: {mode_modes}, with a fraction of {combo_frac} events being combined. The average number of events involved in a combination is: {avg_ev_in_combo}\")\n",
    "    # the mode modes here refer to the labels given to the events by the ai, not the true labels (for reference) \n",
    "    return event_splits, all_misidentified, avg_misidentified, mode_modes, combo_frac, avg_ev_in_combo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Label-Focused Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these results should match up exactly with the previous loss function within rounding errors \n",
    "\n",
    "def ai_based_loss(true_labels, network_labels):\n",
    "    '''This function does the same thing as the previous loss function, but does it in such a way \n",
    "    that the ai labels are ordered and the clustering is based off of these. it should give the same \n",
    "    or similar results to the original loss function'''\n",
    "    \n",
    "    dom_true_labels = []\n",
    "    total_combos = 0 \n",
    "    ev_per_combo = 0 # divide by number of combos to get average\n",
    "    fractions_misIDs = []\n",
    "    total_splits = 0 \n",
    "    ev_per_split = 0 # divide by number of splits to get the average \n",
    "\n",
    "    sorted_pairs = sorted(zip(network_labels, true_labels))\n",
    "    #reordered labels are below\n",
    "    reo_network_labels, reo_true_labels = zip(*sorted_pairs)\n",
    "\n",
    "    break_indices = breaks(reo_network_labels)\n",
    "    gaps = sizes(break_indices)\n",
    "\n",
    "    beginning = 0 \n",
    "    for i in range(len(gaps)):\n",
    "        end = break_indices[i] # determine where the chunk ends \n",
    "        chunk = reo_true_labels[beginning:end]\n",
    "        chunk_modes = modded_mode(chunk) # finding the dominant labels in the chunk (max 3)\n",
    "        dom_true_labels.extend(chunk_modes) # adding to mega list of dominant labels \n",
    "        e_in_combo = len(chunk_modes)\n",
    "        if e_in_combo > 1: \n",
    "            total_combos += 1\n",
    "            ev_per_combo += e_in_combo\n",
    "        \n",
    "        misIDs = sum(1 for item in chunk if item not in chunk_modes)\n",
    "        fractions_misIDs.append(misIDs/gaps[i])\n",
    "        beginning = break_indices[i] # update the beginning of the following chunk\n",
    "    \n",
    "    # totaling shit and averages \n",
    "    counter = Counter(dom_true_labels)\n",
    "    all_counts = dict(counter) # amount of times each label shows up in the list \n",
    "    repeat_labels = {item: count for item, count in counter.items() if count >1}\n",
    "    total_splits = len(repeat_labels.values())\n",
    "    ev_per_split = sum(repeat_labels.values())/total_splits # average number of events in a split \n",
    "    frac_splits = total_splits/len(all_counts.values()) # fraction of events involved in a split \n",
    "\n",
    "    ev_per_combo = ev_per_combo/total_combos # averaging out \n",
    "    avg_misIDs = np.average(fractions_misIDs)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
