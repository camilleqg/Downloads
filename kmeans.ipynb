{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have the same content as the single-run kmeans file but in a jupyter notebook format it'll be easier to run and manage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd  \n",
    "from scipy import stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter \n",
    "import tqdm\n",
    "\n",
    "events = 10 \n",
    "density = '1' \n",
    "noise = 0 \n",
    "filename = None \n",
    "folder = None \n",
    "k = 100\n",
    "max_iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_gpu(data, k, max_iters=100):\n",
    "    data = data.to(device)\n",
    "    centroids = data[torch.randperm(len(data))[:k]]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        #print(f\"Iteration: {i+1}\")\n",
    "        dist = torch.cdist(data, centroids) \n",
    "        ai_labels = torch.argmin(dist, dim=1)\n",
    "\n",
    "        # update centroids \n",
    "        for j in range(k):\n",
    "            centroids[j] = data[ai_labels == j].mean(dim=0)\n",
    "    \n",
    "    return ai_labels.cpu().numpy(), centroids.numpy()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelmaker(events=None, sp_density=None, t_density=None, noise=None, filename = None, folder = None): \n",
    "    '''creates labels based on my naming convention for different files, keeps it consistent and easy'''\n",
    "    if folder: \n",
    "        folder = folder + '/'\n",
    "\n",
    "    if filename: \n",
    "        datafile = str(filename) \n",
    "    else: \n",
    "        datafile = str(events) + 'ev_' + str(sp_density) + 'spd_' + str(t_density) + 'td_n' + str(noise)\n",
    "    \n",
    "\n",
    "    labelfile = 'labels_' + datafile + '.csv'\n",
    "    sourcefile = 'sources_' + datafile + '.csv'\n",
    "    ai_labelfile = datafile + '_results' + '.csv'\n",
    "    centroidfile = datafile + '_centroids' + '.csv'\n",
    "    datafile = datafile + '.csv'\n",
    "\n",
    "    if folder: \n",
    "        datafile = folder + datafile \n",
    "        centroidfile = folder + centroidfile \n",
    "        ai_labelfile = folder + ai_labelfile \n",
    "        labelfile = folder + labelfile \n",
    "        sourcefile = folder + sourcefile \n",
    "\n",
    "    \n",
    "    return datafile, labelfile, sourcefile, ai_labelfile, centroidfile\n",
    "\n",
    "def readfiles(datafile, labelfile, sourcefile): \n",
    "    '''data reader and simplifier for files that haven't been passed through the algorithm'''\n",
    "    columns = ['x[px]', 'y[px]', 't[s]']\n",
    "    \n",
    "    dataread = pd.read_csv(datafile) \n",
    "    data = np.array(dataread[columns])\n",
    "    \n",
    "    labelread = pd.read_csv(labelfile)\n",
    "    labels = np.array(labelread['labels'])\n",
    "\n",
    "    sourceread = pd.read_csv(sourcefile) \n",
    "    sources = np.array(sourceread[columns])\n",
    "\n",
    "    return data, labels, sources\n",
    "\n",
    "def readai(datafile, labelfile, sourcefile, ai_labelfile, centroidfile):\n",
    "    '''file reader and data simplifier for data thats been through the algorithm'''\n",
    "    columns = ['x[px]', 'y[px]', 't[s]']\n",
    "    \n",
    "    dataread = pd.read_csv(datafile) \n",
    "    data = np.array(dataread[columns])\n",
    "    \n",
    "    labelread = pd.read_csv(labelfile)\n",
    "    labels = np.array(labelread['labels'])\n",
    "\n",
    "    sourceread = pd.read_csv(sourcefile) \n",
    "    sources = np.array(sourceread[columns])\n",
    "\n",
    "    ai_labelsread = pd.read_csv(ai_labelfile)\n",
    "    ai_labels = np.array(ai_labelsread['labels'])\n",
    "\n",
    "    centroidread = pd.read_csv(centroidfile) \n",
    "    centroids = np.array(centroidread[columns])\n",
    "\n",
    "    return data, labels, sources, ai_labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating datafile names and data + normalizing it using MinMaxScaling below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafile, labelfile, sourcefile, ai_labelfile, centroidfile = labelmaker(events, density, noise, filename, folder)\n",
    "# # read out the files\n",
    "# data, labels, sources = readfiles(datafile, labelfile, sourcefile)\n",
    "# # establish feature range and transform data, can comment this out to turn off scaling, naming is the same\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# # scale coordinate data \n",
    "# data = scaler.fit_transform(data)\n",
    "# sources = scaler.fit_transform(sources)\n",
    "# # turn data into a tensor \n",
    "# data_tensor = torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_labels, centroids = kmeans_gpu(data_tensor, k, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to the centroid and ai_label files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(ai_labelfile, mode = 'w', newline='') as wfile: \n",
    "#     writer = csv.writer(wfile)\n",
    "#     writer.writerow(['labels'])\n",
    "#     for item in ai_labels: \n",
    "#                 writer.writerow([item])\n",
    "\n",
    "# columns = ['x[px]', 'y[px]', 't[s]']\n",
    "# with open(centroidfile, mode = 'w', newline = '') as wfile: \n",
    "#     writer = csv.writer(wfile)\n",
    "#     writer.writerow(columns)\n",
    "#     writer.writerows(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reading out ai generated files after the algorithm run \n",
    "\n",
    "# data, labels, sources, ai_labels, centroids = readai(datafile, labelfile, sourcefile, ai_labelfile, centroidfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaks and sizing functions for the true label-focused loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaks(array): \n",
    "    '''This function takes an array, goes through it item by item, and returns the list of indices where the value changes'''\n",
    "    value = array[0] \n",
    "    indices = []\n",
    "    for index, ele in enumerate(array): \n",
    "        if ele != value:\n",
    "            indices.append(index)\n",
    "            value = array[index]\n",
    "    indices.append(len(array))\n",
    "\n",
    "    return indices\n",
    "\n",
    "def sizes(indices): \n",
    "    '''This function takes a list of indices and calculates the number of items belonging to each value by taking the difference between \n",
    "    subsequent indices'''\n",
    "    gaps = []\n",
    "    prev=0\n",
    "    for i in range(len(indices)):\n",
    "        gaps.append(indices[i]-prev)\n",
    "        prev = indices[i]\n",
    "\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Label-Focused Loss, ripped from the losses file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional function needed for the loss function \n",
    "def modded_mode(array): \n",
    "    '''This function will return event labels, its a modification on a normal mode function where the label has to appear at least 33% of the time to be a label in that chunk.'''\n",
    "    n = len(array)\n",
    "    if n == 0: \n",
    "        return []\n",
    "    \n",
    "    threshold = n/3 \n",
    "    counts = Counter(array)\n",
    "\n",
    "    result = [key for key, count in counts.items() if count >= threshold]\n",
    "    return result \n",
    "\n",
    "# THE loss func. \n",
    "\n",
    "def truth_based_loss(true_labels, ai_labels):\n",
    "    # labels are already sorted by true labels predictions \n",
    "\n",
    "    # getting break indices and cluster sizes \n",
    "    break_indices = breaks(true_labels)\n",
    "    gaps = sizes(break_indices)\n",
    "    n_events = len(gaps)\n",
    "\n",
    "    # initialize variables \n",
    "    counter = Counter()\n",
    "    fractions_misIDs = []\n",
    "    total_splits, ev_per_split = 0,0 \n",
    "    total_splits = 0 \n",
    "    total_misIDs = 0 \n",
    "\n",
    "\n",
    "    # process each chunk that's separated by truth gaps \n",
    "    for start, end, gap in zip([0] + break_indices[:-1], break_indices, gaps):\n",
    "        chunk = ai_labels[start:end]\n",
    "        chunk_modes = modded_mode(chunk)\n",
    "\n",
    "        # update counter\n",
    "        counter.update(chunk_modes)\n",
    "    \n",
    "        e_in_split = len(chunk_modes) # number of ai events found in the chunk\n",
    "        if e_in_split > 1: \n",
    "            total_splits += 1\n",
    "            ev_per_split += e_in_split # if there is more than one, increase splits and events in split \n",
    "        \n",
    "        misIDs = sum(1 for item in chunk if item not in chunk_modes)\n",
    "        total_misIDs += misIDs\n",
    "        fractions_misIDs.append(misIDs/gap)\n",
    "    \n",
    "    # Combination Statistics \n",
    "    repeat_labels = {k: v for k, v in counter.items() if v>1}\n",
    "    total_combos = len(repeat_labels) # the amount of combinations is the same as the amount of labels that get repeated through the set \n",
    "    ev_per_combo = sum(repeat_labels.values())/total_combos if total_combos else 0 # sum all repeats together and average over number of combinations \n",
    "    frac_combos = total_combos / n_events # fraction of events experiencing combination\n",
    "    \n",
    "    # other stats \n",
    "    ev_per_split = ev_per_split/total_splits if total_splits else 0\n",
    "    frac_splits = total_splits/n_events\n",
    "    avg_misIDs = np.mean(fractions_misIDs)\n",
    "\n",
    "    # output da resultssss\n",
    "    # print(f\"The fraction of splits over all events is {frac_splits}\")\n",
    "    # print(f\"The average number of events involved in a single split is {ev_per_split}\")\n",
    "    # print(f\"The fraction of combinations over all events is {frac_combos}\")\n",
    "    # print(f\"The average number of events involved in a single combo is {ev_per_combo}\")\n",
    "    # print(f\"The average fraction of photons misidentified in each event is {avg_misIDs}\")\n",
    "\n",
    "    return frac_splits, ev_per_split, frac_combos, ev_per_combo, avg_misIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Label-Focused Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these results should match up exactly with the previous loss function within rounding errors \n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def ai_based_loss(true_labels, network_labels):\n",
    "    \"\"\"\n",
    "    More efficient implementation of the AI-based loss function.\n",
    "    This function clusters AI labels and calculates relevant loss statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort labels based on AI predictions\n",
    "    sorted_pairs = sorted(zip(network_labels, true_labels))\n",
    "    reo_network_labels, reo_true_labels = zip(*sorted_pairs)\n",
    "\n",
    "    # Get break indices and cluster sizes\n",
    "    break_indices = breaks(reo_network_labels)\n",
    "    gaps = sizes(break_indices)\n",
    "    # n_events = len(gaps)\n",
    "\n",
    "    # Initialize variables\n",
    "    counter = Counter()\n",
    "    fractions_misIDs = []\n",
    "    total_combos, ev_per_combo = 0, 0\n",
    "    total_splits = 0\n",
    "    total_misIDs = 0 \n",
    "\n",
    "    # Process each AI-clustered chunk\n",
    "    for start, end, gap in zip([0] + break_indices[:-1], break_indices, gaps):\n",
    "        chunk = reo_true_labels[start:end]\n",
    "        chunk_modes = modded_mode(chunk)  # Get dominant modes (max 3)\n",
    "        \n",
    "        # Update Counter directly  \n",
    "        counter.update(chunk_modes)\n",
    "\n",
    "        e_in_combo = len(chunk_modes)\n",
    "        if e_in_combo > 1:\n",
    "            total_combos += 1\n",
    "            ev_per_combo += e_in_combo\n",
    "        \n",
    "        misIDs = sum(1 for item in chunk if item not in chunk_modes)\n",
    "        total_misIDs += misIDs\n",
    "        fractions_misIDs.append(misIDs / gap)\n",
    "    \n",
    "    n_events = len(counter)\n",
    "    # Compute split statistics\n",
    "    repeat_labels = {k: v for k, v in counter.items() if v > 1}\n",
    "    total_splits = len(repeat_labels)\n",
    "    ev_per_split = sum(repeat_labels.values()) / total_splits if total_splits else 0\n",
    "    frac_splits = total_splits / n_events\n",
    "\n",
    "    # Compute final metrics\n",
    "    ev_per_combo = ev_per_combo / total_combos if total_combos else 0\n",
    "    frac_combos = total_combos / n_events\n",
    "    avg_misIDs = np.mean(fractions_misIDs)\n",
    "\n",
    "\n",
    "    # Output results\n",
    "    print(f\"The fraction of splits over all events is {frac_splits}\")\n",
    "    print(f\"The average number of events involved in a single split is {ev_per_split}\")\n",
    "    print(f\"The fraction of combinations over all events is {frac_combos}\")\n",
    "    print(f\"The average number of events involved in a single combo is {ev_per_combo}\")\n",
    "    print(f\"The average fraction of photons misidentified in each event is {avg_misIDs}\")\n",
    "\n",
    "    return frac_splits, ev_per_split, frac_combos, ev_per_combo, avg_misIDs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing to compare ai and truth based loss functions \n",
    "# print(\"---- TRUTH BASED LOSS: ----\")\n",
    "# truth_based_loss(labels, ai_labels)\n",
    "# print(\"---- AI BASED LOSS: ----\")\n",
    "# ai_based_loss(labels, ai_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test dataset for loss function testing \n",
    "\n",
    "# true_labelz = np.array([1,1,1,1,2,2,3,3,3,3,3,3])\n",
    "# ai_labelz = np.array([8,55,55,55,55,55,101,101,8,8,55,8])\n",
    "\n",
    "# truth_based_loss(true_labelz, ai_labelz)\n",
    "# print(\"----\")\n",
    "# ai_based_loss(true_labelz, ai_labelz)\n",
    "\n",
    "# # (8, 8, 8, 8, 55, 55, 55, 55, 55, 55, 101, 101)\n",
    "# # (1, 3, 3, 3, 1, 1, 1, 2, 2, 3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELBOW METHOD \n",
    "def elbowmeth(int_range, datafile, folder=None, iterations=None, pltfile=None, pltnum=None, weighted = True):\n",
    "    '''will perform the elbow method on a given dataset for a given range of k integers and return the k value with the lowest loss, as well as the value of that loss\n",
    "    and a plot demonstrating the loss'''\n",
    "    datafile, labelfile, sourcefile, ai_labelfile, centroidfile = labelmaker(filename = datafile, folder = folder)\n",
    "    data, labels, sources = readfiles(datafile, labelfile, sourcefile)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    # scale coordinate data \n",
    "    data = scaler.fit_transform(data)\n",
    "    sources = scaler.fit_transform(sources)\n",
    "    # turn data into a tensor \n",
    "    data_tensor = torch.Tensor(data)\n",
    "    total_losses = []    \n",
    "\n",
    "    coeff = [1,2,3] if weighted else [1,1,1]\n",
    "\n",
    "    for kval in int_range: \n",
    "        ai_labels, centroids = kmeans_gpu(data_tensor, k=kval, max_iters=iterations)\n",
    "        frac_splits, ev_per_split, frac_combos, ev_per_combo, avg_misIDs = truth_based_loss(true_labels = labels, ai_labels=ai_labels)\n",
    "        if weighted: \n",
    "            coeff = [1,2,3]\n",
    "        total_loss = np.dot(coeff, [avg_misIDs, frac_splits, frac_combos])\n",
    "        total_losses.append(total_loss)\n",
    "\n",
    "    if pltnum%5 == 0:\n",
    "        #generate plot for every fifth iteration\n",
    "        plt.figure()\n",
    "        plt.plot(int_range, total_losses)\n",
    "        plt.xlabel(\"K values\")\n",
    "        plt.ylabel(\"Total loss\")\n",
    "        plt.title(f\"Elbow method for k = {int_range} for {datafile} {pltnum}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(pltfile)   \n",
    "        print(f\"{pltnum}% complete!\")     \n",
    "\n",
    "    min_loss = np.min(total_losses)\n",
    "    min_k = int_range[total_losses.index(min_loss)]\n",
    "\n",
    "    print(f\"The minimum loss is {min_loss} with a k value of {min_k}\")\n",
    "    return min_loss, min_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmentation_sim import new_parallel_sim # this only works if parsed arguments in segmentation_sim.py are commented out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Suppress stdout temporarily\n",
    "class SuppressPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#big algo \n",
    "\n",
    "k_range = [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105]\n",
    "bigLossFile = '0.5temp100x100/UWloss_totals.csv'\n",
    "columns = ['lowest kval', 'min loss']\n",
    "\n",
    "\n",
    "with open(bigLossFile, mode = 'w', newline = '') as wfile: \n",
    "    writer = csv.writer(wfile)\n",
    "    writer.writerow(columns) # write the columns into the loss file to begin with \n",
    "\n",
    "for i in range(100): # 100 iterations to get good statistics \n",
    "    with SuppressPrints():\n",
    "        new_parallel_sim(100, 0, 10, t_density = '.5', folder = '0.5temp100x100', dataSaveID = 'UWtemp') # generate data of 100 events, no noise, full density, using 5 cores for speed \n",
    "    \n",
    "    print(f\"{i}th datafile generated!\")\n",
    "    min_loss, min_k = elbowmeth(k_range, datafile = 'UWtemp', folder = '0.5temp100x100', iterations = 100, pltfile = f'0.5temp100x100/UWkvals_losses_{i}.png', pltnum = i, weighted = False) # run elbow meth!\n",
    "    print(f\"{i}th losses and k values are: {min_loss}, {min_k}\")\n",
    "    with open(bigLossFile, mode = 'a', newline = '') as wfile: # write in the best loss and best kval \n",
    "        writer = csv.writer(wfile)\n",
    "        writer.writerow([min_k, min_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read out the two temp 100x100 files \n",
    "# plot histogram \n",
    "# calculate averages and stds\n",
    "\n",
    "filenameW = 'temp100x100/loss_totals.csv'\n",
    "columns = ['lowest kval', 'min loss']\n",
    "datareadW = pd.read_csv(filenameW)\n",
    "Wdata = np.array(datareadW[columns])\n",
    "\n",
    "filenameUW = 'temp100x100/UWloss_totals.csv'\n",
    "datareadUW = pd.read_csv(filenameUW)\n",
    "UWdata = np.array(datareadUW[columns])\n",
    "\n",
    "avg_W_loss = np.mean(Wdata[:,1])\n",
    "std_W_loss = np.std(Wdata[:,1])\n",
    "avg_W_k = np.mean(Wdata[:,0])\n",
    "std_W_k = np.std(Wdata[:,0])\n",
    "\n",
    "avg_UW_loss = np.mean(UWdata[:,1])\n",
    "std_UW_loss = np.std(UWdata[:,1])\n",
    "avg_UW_k = np.mean(UWdata[:,0])\n",
    "std_UW_k = np.std(UWdata[:,0])\n",
    "\n",
    "with open(file = 'temp100x100/results.csv', mode = 'w') as wfile: \n",
    "    writer = csv.writer(wfile)\n",
    "    writer.writerow([\"value\", \"avg\", \"std\"])\n",
    "    writer.writerow([\"W loss\", avg_W_loss, std_W_loss])\n",
    "    writer.writerow([\"W kval\", avg_W_k, std_W_k])\n",
    "    writer.writerow([\"UW loss\", avg_UW_loss, std_UW_loss])\n",
    "    writer.writerow([\"UW kval\", avg_UW_k, std_UW_k])\n",
    "# plt.figure()\n",
    "# plt.scatter(data[:,0], data[:,1])\n",
    "# plt.xlabel(\"Best k value\")\n",
    "# plt.ylabel(\"Minimum total weighted loss\")\n",
    "# plt.title(\"Best k value and associated minimum unweighted loss for 100 datasets \\nof 100 events with 50% temporal density\")\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"0.5temp100x100/UWkval_vs_loss.png\")\n",
    "\n",
    "# plt.figure()\n",
    "# plt.hist(data[:,0], bins = 11, edgecolor = 'black', alpha = 0.7)\n",
    "# plt.xlabel(\"Best K value\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# plt.title(\"Histogram of best unweighted k values for 100 datasets \\nof 100 events with 50% temporal density\")\n",
    "# plt.savefig(\"0.5temp100x100/UWkval_hist.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
