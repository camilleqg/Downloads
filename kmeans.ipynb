{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have the same content as the single-run kmeans file but in a jupyter notebook format it'll be easier to run and manage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import csv\n",
    "import pandas as pd  \n",
    "from scipy import stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from collections import Counter \n",
    "\n",
    "events = 10 \n",
    "density = '1' \n",
    "noise = 0 \n",
    "filename = None \n",
    "folder = None \n",
    "k = 100\n",
    "max_iters = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_gpu(data, k, max_iters=100):\n",
    "    data = data.to(device)\n",
    "    centroids = data[torch.randperm(len(data))[:k]]\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        print(f\"Iteration: {i+1}\")\n",
    "        dist = torch.cdist(data, centroids) \n",
    "        ai_labels = torch.argmin(dist, dim=1)\n",
    "\n",
    "        # update centroids \n",
    "        for j in range(k):\n",
    "            centroids[j] = data[ai_labels == j].mean(dim=0)\n",
    "    \n",
    "    return ai_labels.cpu().numpy(), centroids.numpy()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelmaker(events, density, noise, filename = None, folder = None): \n",
    "    '''creates labels based on my naming convention for different files, keeps it consistent and easy'''\n",
    "    if folder: \n",
    "        folder = folder + '/'\n",
    "\n",
    "    if filename: \n",
    "        datafile = str(filename) \n",
    "    else: \n",
    "        datafile = str(events) + 'ev_' + str(density) + 'dense_n' + str(noise)\n",
    "    \n",
    "    datafile = datafile + '.csv'\n",
    "    labelfile = 'labels_' + datafile + '.csv'\n",
    "    sourcefile = 'sources_' + datafile + '.csv'\n",
    "    ai_labelfile = datafile + '_results' + '.csv'\n",
    "    centroidfile = datafile + '_centroids' + '.csv'\n",
    "\n",
    "    if folder: \n",
    "        datafile = folder + datafile \n",
    "        centroidfile = folder + centroidfile \n",
    "        ai_labelfile = folder + ai_labelfile \n",
    "        labelfile = folder + labelfile \n",
    "        sourcefile = folder + sourcefile \n",
    "    \n",
    "    return datafile, labelfile, sourcefile, ai_labelfile, centroidfile\n",
    "\n",
    "def readfiles(datafile, labelfile, sourcefile): \n",
    "    '''data reader and simplifier for files that haven't been passed through the algorithm'''\n",
    "    columns = ['x[px]', 'y[px]', 't[s]']\n",
    "    \n",
    "    dataread = pd.read_csv(datafile) \n",
    "    data = np.array(dataread[columns])\n",
    "    \n",
    "    labelread = pd.read_csv(labelfile)\n",
    "    labels = np.array(labelread['labels'])\n",
    "\n",
    "    sourceread = pd.read_csv(sourcefile) \n",
    "    sources = np.array(sourceread[columns])\n",
    "\n",
    "    return data, labels, sources\n",
    "\n",
    "def readai(datafile, labelfile, sourcefile, ai_labelfile, centroidfile):\n",
    "    '''file reader and data simplifier for data thats been through the algorithm'''\n",
    "    columns = ['x[px]', 'y[px]', 't[s]']\n",
    "    \n",
    "    dataread = pd.read_csv(datafile) \n",
    "    data = np.array(dataread[columns])\n",
    "    \n",
    "    labelread = pd.read_csv(labelfile)\n",
    "    labels = np.array(labelread['labels'])\n",
    "\n",
    "    sourceread = pd.read_csv(sourcefile) \n",
    "    sources = np.array(sourceread[columns])\n",
    "\n",
    "    ai_labelsread = pd.read_csv(ai_labelfile)\n",
    "    ai_labels = np.array(ai_labelsread['labels'])\n",
    "\n",
    "    centroidread = pd.read_csv(centroidfile) \n",
    "    centroids = np.array(centroidread[columns])\n",
    "\n",
    "    return data, labels, sources, ai_labels, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating datafile names and data + normalizing it using MinMaxScaling below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"/home/cgillesp/.vscode-server/extensions/ms-python.python-2025.0.0-linux-x64/python_files/python_server.py\", line 133, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"<string>\", line 3, in <module>\n",
       "  File \"<string>\", line 29, in readfiles\n",
       "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1026, in read_csv\n",
       "    return _read(filepath_or_buffer, kwds)\n",
       "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 620, in _read\n",
       "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
       "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1620, in __init__\n",
       "    self._engine = self._make_engine(f, self.engine)\n",
       "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
       "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/site-packages/pandas/io/parsers/readers.py\", line 1880, in _make_engine\n",
       "    self.handles = get_handle(\n",
       "                   ^^^^^^^^^^^\n",
       "  File \"/home/cgillesp/anaconda3/envs/torch/lib/python3.12/site-packages/pandas/io/common.py\", line 873, in get_handle\n",
       "    handle = open(\n",
       "             ^^^^^\n",
       "FileNotFoundError: [Errno 2] No such file or directory: '10ev_1dense_n0'\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datafile, labelfile, sourcefile, ai_labelfile, centroidfile = labelmaker(events, density, noise, filename, folder)\n",
    "# read out the files\n",
    "data, labels, sources = readfiles(datafile, labelfile, sourcefile)\n",
    "# establish feature range and transform data, can comment this out to turn off scaling, naming is the same\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "data = scaler.fit_transform(data)\n",
    "sources = scaler.fit_transform(sources)\n",
    "# turn into a tensor \n",
    "data_tensor = torch.Tensor(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running k-means algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_labels, centroids = kmeans_gpu(data_tensor, k, max_iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing to the centroid and ai_label files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ai_labelfile, mode = 'w', newline='') as wfile: \n",
    "    writer = csv.writer(wfile)\n",
    "    writer.writerow(['labels'])\n",
    "    for item in ai_labels: \n",
    "                writer.writerow([item])\n",
    "\n",
    "columns = ['x[px]', 'y[px]', 't[s]']\n",
    "with open(centroidfile, mode = 'w', newline = '') as wfile: \n",
    "    writer = csv.writer(wfile)\n",
    "    writer.writerow(columns)\n",
    "    writer.writerows(centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaks and sizing functions for the true label-focused loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breaks(array): \n",
    "    '''This function takes an array, goes through it item by item, and returns the list of indices where the value changes'''\n",
    "    value = array[0] \n",
    "    indices = []\n",
    "    for index, ele in enumerate(array): \n",
    "        if ele != value:\n",
    "            indices.append(index)\n",
    "            value = array[index]\n",
    "    indices.append(len(array))\n",
    "\n",
    "    return indices\n",
    "\n",
    "def sizes(indices): \n",
    "    '''This function takes a list of indices and calculates the number of items belonging to each value by taking the difference between \n",
    "    subsequent indices'''\n",
    "    gaps = []\n",
    "    prev=0\n",
    "    for i in range(len(indices)):\n",
    "        gaps.append(indices[i]-prev)\n",
    "        prev = indices[i]\n",
    "\n",
    "    return gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True Label-Focused Loss, ripped from the losses file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional function needed for the loss function \n",
    "def modded_mode(array): \n",
    "    '''This function will return event labels, its a modification on a normal mode function where the label has to appear at least 33% of the time to be a label in that chunk.'''\n",
    "    n = len(array)\n",
    "    if n == 0: \n",
    "        return []\n",
    "    \n",
    "    threshold = n/3 \n",
    "    counts = Counter(array)\n",
    "\n",
    "    result = [key for key, count in counts.items() if count > threshold]\n",
    "    return result \n",
    "\n",
    "# THE loss func. \n",
    "\n",
    "def truth_based_loss(true_labels, network_labels):\n",
    "    # labels are already sorted by true labels predictions \n",
    "\n",
    "    # getting break indices and cluster sizes \n",
    "    break_indices = breaks(true_labels)\n",
    "    gaps = sizes(break_indices)\n",
    "    n_events = len(gaps)\n",
    "\n",
    "    # initialize variables \n",
    "    counter = Counter()\n",
    "    fractions_misIDs = []\n",
    "    total_splits, ev_per_split = 0,0 \n",
    "    total_splits = 0 \n",
    "\n",
    "\n",
    "    # process each chunk that's separated by truth gaps \n",
    "    for start, end, gap in zip([0] + break_indices[:-1], break_indices, gaps)\n",
    "        chunk = ai_labels[start:end]\n",
    "        chunk_modes = modded_mode(chunk)\n",
    "\n",
    "        # update counter\n",
    "        counter.update(chunk_modes)\n",
    "\n",
    "        e_in_split = len(chunk_modes) # number of ai events found in the chunk\n",
    "        if e_in_split > 1: \n",
    "            total_splits += 1\n",
    "            ev_per_split += e_in_split # if there is more than one, increase splits and events in split \n",
    "        \n",
    "        misIDs = sum(1 for item in chunk if item not in chunk_modes)\n",
    "        fractions_misIDs.append(misIDs/gap)\n",
    "\n",
    "    \n",
    "    # Combination Statistics \n",
    "    repeat_labels = {k: v for k, v in counter.items() if v>1}\n",
    "    total_combos = len(repeat_labels) # the amount of combinations is the same as the amount of labels that get repeated through the set \n",
    "    ev_per_combo = sum(repeat_labels.values())/total_combos if total_combos else 0 # sum all repeats together and average over number of combinations \n",
    "    frac_combos = total_combos / n_events # fraction of events experiencing combination\n",
    "\n",
    "    # other stats \n",
    "    ev_per_split = ev_per_split/total_splits if total_splits else 0\n",
    "    frac_splits = total_splits/n_events\n",
    "    avg_misIDs = np.mean(fractions_misIDs)\n",
    "\n",
    "    # output da resultssss\n",
    "    print(f\"The fraction of splits over all events is {frac_splits}\")\n",
    "    print(f\"The average number of events involved in a single split is {ev_per_split}\")\n",
    "    print(f\"The fraction of combinations over all events is {frac_combos}\")\n",
    "    print(f\"The average number of events involved in a single combo is {ev_per_combo}\")\n",
    "    print(f\"The average number of photons misidentified in each event is {avg_misIDs}\")\n",
    "\n",
    "    return frac_splits, ev_per_split, frac_combos, ev_per_combo, avg_misIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Label-Focused Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these results should match up exactly with the previous loss function within rounding errors \n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def ai_based_loss(true_labels, network_labels):\n",
    "    \"\"\"\n",
    "    More efficient implementation of the AI-based loss function.\n",
    "    This function clusters AI labels and calculates relevant loss statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort labels based on AI predictions\n",
    "    sorted_pairs = sorted(zip(network_labels, true_labels))\n",
    "    reo_network_labels, reo_true_labels = zip(*sorted_pairs)\n",
    "\n",
    "    # Get break indices and cluster sizes\n",
    "    break_indices = breaks(reo_network_labels)\n",
    "    gaps = sizes(break_indices)\n",
    "    n_events = len(gaps)\n",
    "\n",
    "    # Initialize variables\n",
    "    counter = Counter()\n",
    "    fractions_misIDs = []\n",
    "    total_combos, ev_per_combo = 0, 0\n",
    "    total_splits = 0\n",
    "\n",
    "    # Process each AI-clustered chunk\n",
    "    for start, end, gap in zip([0] + break_indices[:-1], break_indices, gaps):\n",
    "        chunk = reo_true_labels[start:end]\n",
    "        chunk_modes = modded_mode(chunk)  # Get dominant modes (max 3)\n",
    "        \n",
    "        # Update Counter directly  \n",
    "        counter.update(chunk_modes)\n",
    "\n",
    "        e_in_combo = len(chunk_modes)\n",
    "        if e_in_combo > 1:\n",
    "            total_combos += 1\n",
    "            ev_per_combo += e_in_combo\n",
    "        \n",
    "        misIDs = sum(1 for item in chunk if item not in chunk_modes)\n",
    "        fractions_misIDs.append(misIDs / gap)\n",
    "\n",
    "    # Compute split statistics\n",
    "    repeat_labels = {k: v for k, v in counter.items() if v > 1}\n",
    "    total_splits = len(repeat_labels)\n",
    "    ev_per_split = sum(repeat_labels.values()) / total_splits if total_splits else 0\n",
    "    frac_splits = total_splits / n_events\n",
    "\n",
    "    # Compute final metrics\n",
    "    ev_per_combo = ev_per_combo / total_combos if total_combos else 0\n",
    "    frac_combos = total_combos / n_events\n",
    "    avg_misIDs = np.mean(fractions_misIDs)\n",
    "\n",
    "    # Output results\n",
    "    print(f\"The fraction of splits over all events is {frac_splits}\")\n",
    "    print(f\"The average number of events involved in a single split is {ev_per_split}\")\n",
    "    print(f\"The fraction of combinations over all events is {frac_combos}\")\n",
    "    print(f\"The average number of events involved in a single combo is {ev_per_combo}\")\n",
    "    print(f\"The average number of photons misidentified in each event is {avg_misIDs}\")\n",
    "\n",
    "    return frac_splits, ev_per_split, frac_combos, ev_per_combo, avg_misIDs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing to compare ai and truth based loss functions \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
